{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890b373f",
   "metadata": {},
   "source": [
    "# Fine Tuning Arabertv2\n",
    "We will try finetuning arabertv2 on our dataset (MADAR) for it to identify arabic dialects based on region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67846c57",
   "metadata": {},
   "source": [
    "#### Tasks to do:\n",
    "- Load Dataset ✔️\n",
    "- Prepare the dataset ✔️\n",
    "- Convert to Hugging Face Dataset ✔️\n",
    "- Tokenization ✔️\n",
    "- Model setup\n",
    "- Training loop\n",
    "- Run training\n",
    "- Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1794211",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "71447714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eac83a9",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "1. Loading dataset\n",
    "2. Mapping each country to region\n",
    "3. Limiting each region to 11,0000 sentences to balance it (6 regions and MSA so 77,000 sentences)\n",
    "4. Shuffling the dataset so its random (random_state=42 for recreatability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1881f15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AE', 'BH', 'DZ', 'EG', 'IQ', 'JO', 'KW', 'LB', 'LY', 'MA', 'OM',\n",
       "       'PL', 'QA', 'SA', 'SD', 'SY', 'TN', 'YE', 'MSA'], dtype=object)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading dataset\n",
    "dataset = pd.read_csv(\"Dataset/data_v0.1.0.csv\")\n",
    "\n",
    "dataset[\"dialect\"].unique() # All the unique dialects in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fa3733c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "regions\n",
       "GULF    11000\n",
       "IRAQ    11000\n",
       "LEV     11000\n",
       "MSA     11000\n",
       "NA      11000\n",
       "NILE    11000\n",
       "YEM     11000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Map each country to region\n",
    "country_to_region = {\n",
    "    \"AE\" : \"GULF\", \n",
    "    \"BH\" : \"GULF\",\n",
    "    \"DZ\" : \"NA\", \n",
    "    \"EG\" : \"NILE\",\n",
    "    \"IQ\" : \"IRAQ\",\n",
    "    \"JO\" : \"LEV\",\n",
    "    \"KW\" : \"GULF\",\n",
    "    \"LB\" : \"LEV\",\n",
    "    \"LY\" : \"NA\",\n",
    "    \"MA\" : \"NA\", \n",
    "    \"OM\" : \"GULF\",\n",
    "    \"PL\" : \"LEV\", \n",
    "    \"QA\" : \"GULF\",\n",
    "    \"SA\" : \"GULF\",\n",
    "    \"SD\" : \"NILE\", \n",
    "    \"SY\" :  \"LEV\",\n",
    "    \"TN\" : \"NA\",\n",
    "    \"YE\" : \"YEM\",\n",
    "    \"MSA\" : \"MSA\"\n",
    "}\n",
    "dataset[\"regions\"] = dataset[\"dialect\"].map(country_to_region) # mapping dialect to new column called Region\n",
    "dataset = dataset.groupby(\"regions\",group_keys=False).sample(n=11000, random_state=42) #grouping each Region, limiting each to 11,000 then shuffling it\n",
    "dataset.drop(columns=\"dialect\", inplace=True) # remove dialect column as we dont need it\n",
    "dataset[\"regions\"].value_counts() # Regions and amount of each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "95e336b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>regions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>193840</th>\n",
       "      <td>في العالم كله ماشي حرب صارت ما تندمو عليها راج...</td>\n",
       "      <td>GULF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189271</th>\n",
       "      <td>براءتهم قبل كل شي وش ذنبهم اله يغفرلهم</td>\n",
       "      <td>GULF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125177</th>\n",
       "      <td>حبيب قلبي انت عيل اعتذر لك لأني قرأت التغريده ...</td>\n",
       "      <td>GULF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7168</th>\n",
       "      <td>افخم جزراويه على وجه الارض</td>\n",
       "      <td>GULF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29618</th>\n",
       "      <td>دام يدفع مافي مشكله</td>\n",
       "      <td>GULF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text regions\n",
       "193840  في العالم كله ماشي حرب صارت ما تندمو عليها راج...    GULF\n",
       "189271             براءتهم قبل كل شي وش ذنبهم اله يغفرلهم    GULF\n",
       "125177  حبيب قلبي انت عيل اعتذر لك لأني قرأت التغريده ...    GULF\n",
       "7168                           افخم جزراويه على وجه الارض    GULF\n",
       "29618                                 دام يدفع مافي مشكله    GULF"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3fd287",
   "metadata": {},
   "source": [
    "get all unique regions and make a dictionary for the labels and ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cd8d4129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = dataset[\"regions\"].unique()\n",
    "label2id = {l: i for i, l in enumerate(labels)}\n",
    "id2label = {i: l for i, l in enumerate(labels)}\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32113772",
   "metadata": {},
   "source": [
    "#### Converting Dataset to Hugging face Dataset\n",
    "- converting it from pandas to Hugging Face Dataset\n",
    "- splitting it 80% training and 20% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "80474757",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Dataset.from_pandas(dataset,preserve_index=False)\n",
    "\n",
    "df = df.train_test_split(test_size=0.2,seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ad266a",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "A tokenizer ✂️ is a tool that breaks down sentences into smaller pieces called tokens such as words or parts of words. For example, \"I love dogs\" can be turned into [\"I\", \"love\", \"dogs\"] to help computers understand text.\n",
    "Then it turns the tokens into numbers (IDs) that a machine learning model can use. There are different types of tokenizers, like word, subword, or character tokenizers, depending on the task.\n",
    "We will use the tokenizer that came with the ArabertV2 which was designed for arabic text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "037a9026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 61600/61600 [00:42<00:00, 1447.76 examples/s] \n",
      "Map: 100%|██████████| 15400/15400 [00:00<00:00, 29720.05 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabert\")\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"],truncation=True)\n",
    "\n",
    "tokenized = df.map(tokenize,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "12a52c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'regions', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 61600\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'regions', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 15400\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8c2e27be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 61600/61600 [00:02<00:00, 23761.86 examples/s]\n",
      "Map: 100%|██████████| 15400/15400 [00:00<00:00, 25661.27 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 61600\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 15400\n",
      "    })\n",
      "})\n",
      "['input_ids', 'token_type_ids', 'attention_mask', 'labels']\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# You currently have: DatasetDict(train/test) with columns: \n",
    "# ['text', 'Region', 'input_ids', 'token_type_ids', 'attention_mask']\n",
    "\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# 1) Define a STABLE label order (don’t rely on set/unique order)\n",
    "labels = [\"GULF\", \"LEV\", \"NA\", \"NILE\", \"IRAQ\", \"YEM\", \"MSA\"]\n",
    "label2id = {l:i for i,l in enumerate(labels)}\n",
    "id2label = {i:l for l,i in label2id.items()}\n",
    "\n",
    "# 2) Add integer labels to both splits (without re-tokenizing)\n",
    "def add_labels(batch):\n",
    "    return {\"labels\": label2id[batch[\"regions\"]]}\n",
    "\n",
    "tokenized = tokenized.map(add_labels)  # applies to both train and test in a DatasetDict\n",
    "\n",
    "# 3) (Optional) keep only the columns the model needs\n",
    "keep_cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "if \"token_type_ids\" in tokenized[\"train\"].column_names:\n",
    "    keep_cols.append(\"token_type_ids\")\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": tokenized[\"train\"].remove_columns([c for c in tokenized[\"train\"].column_names if c not in keep_cols]),\n",
    "    \"test\":  tokenized[\"test\"].remove_columns([c for c in tokenized[\"test\"].column_names  if c not in keep_cols]),\n",
    "})\n",
    "\n",
    "# 4) (Optional) set PyTorch format for speed\n",
    "dataset.set_format(type=\"torch\", columns=keep_cols)\n",
    "\n",
    "# 5) Sanity checks\n",
    "print(dataset)\n",
    "print(dataset[\"train\"].column_names)  # must include 'labels'\n",
    "print(dataset[\"train\"][0].keys())     # must include 'labels'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47f780a",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2e6e08c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"aubmindlab/bert-base-arabert\",\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2af45f6",
   "metadata": {},
   "source": [
    "### Training Setup\n",
    "\n",
    "1. **Data Collator**  \n",
    "   Ensures that batches are padded dynamically to the longest sequence in each batch, making training efficient.\n",
    "\n",
    "2. **Metrics**  \n",
    "   We'll evaluate the model using **accuracy**, since our task is multi-class dialect classification.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "48dce51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"f1_macro\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890314ee",
   "metadata": {},
   "source": [
    "\n",
    "### Training Arguments\n",
    "\n",
    "Here we define the **hyperparameters** and **training configuration**:\n",
    "- `evaluation_strategy=\"epoch\"` → Evaluate after each epoch.  \n",
    "- `save_strategy=\"epoch\"` → Save model checkpoints every epoch.  \n",
    "- `learning_rate=2e-5` → Standard fine-tuning learning rate.  \n",
    "- `num_train_epochs=5` → Train for 5 epochs.  \n",
    "- `load_best_model_at_end=True` → Keeps the best model according to our metric (**f1_macro**).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d84cc982",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    fp16=True,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    dataloader_num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c6fd52ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\essah\\AppData\\Local\\Temp\\ipykernel_8972\\4080381228.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "140623f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3190721273422241, 'eval_accuracy': 0.692987012987013, 'eval_f1_macro': 0.693926644954301, 'eval_runtime': 34.2247, 'eval_samples_per_second': 449.968, 'eval_steps_per_second': 14.083, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c9a7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2618' max='19250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2618/19250 03:06 < 19:44, 14.05 it/s, Epoch 0.68/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34689064",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate(tokenized[\"test\"])\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1009fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "\n",
    "def predict_one(text):\n",
    "    # preprocess if you use Arabic normalization\n",
    "    enc = tokenizer(text, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits\n",
    "    probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "    pred_id = int(np.argmax(probs))\n",
    "    return id2label[pred_id], float(probs[pred_id])\n",
    "\n",
    "# Example usage\n",
    "print(predict_one(\"يا حبيبي كيفك؟\"))   # should map to LEV\n",
    "print(predict_one(\"إزيك عامل إيه؟\"))   # should map to NILE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
